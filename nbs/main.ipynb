{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR VS IS Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from statistics import mode\n",
    "import openpyxl\n",
    "import tqdm\n",
    "\n",
    "import sys \n",
    "import joblib\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "sys.path.append('H:/Documents/PhD/3rd-year-project/classify-mosquitoes/src/')\n",
    "import extract, split, config\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Size and Overlap (in seconds)\n",
    "segment_size = 7.5\n",
    "segment_overlap = 6.5\n",
    "\n",
    "# Trials split between test/train and validation set\n",
    "test_trials = np.array([2,3 ,6,7,8, 11,12, 15,16])\n",
    "target_trials = np.array([1,1, 0,0,0, 0,0, 1,1])\n",
    "hyp_trials = np.array([0,1, 4,5, 9,10, 13,14])\n",
    "\n",
    "# Paths\n",
    "results_path = config.PATH + 'tuned model/random-forests/' # Results stored\n",
    "data_path = results_path + 'data/' # Any data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks, trackTargets, tracksTrialId = extract.load(config.FILE, config.PATH, config.IS_RESISTANT, config.DATA_PATH)\n",
    "\n",
    "with open(data_path + 'raw_tracks.npy', 'wb') as w:\n",
    "    np.save(w, np.array(tracks, dtype=object))\n",
    "with open(data_path + 'raw_trackTargets.npy', 'wb') as w:\n",
    "    np.save(w, np.array(trackTargets, dtype=object))\n",
    "with open(data_path + 'raw_tracksTrialId.npy', 'wb') as w:\n",
    "    np.save(w, np.array(tracksTrialId, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(data_path + 'raw_tracks.npy', allow_pickle=True)\n",
    "trackTargets = np.load(data_path + 'raw_trackTargets.npy', allow_pickle=True)\n",
    "tracksTrialId = np.load(data_path + 'raw_tracksTrialId.npy', allow_pickle=True)   \n",
    "\n",
    "tracks = extract.generate_features(tracks, (0,1), 2)\n",
    "\n",
    "with open(data_path + 'tracks_features.npy', 'wb') as w:\n",
    "    np.save(w, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(data_path + 'tracks_features.npy', allow_pickle=True)\n",
    "bmodes = joblib.load('bmodess.dat')\n",
    "track_id = 0\n",
    "while track_id < len(tracks):\n",
    "    mask = np.isin(tracks[track_id][:, 16], bmodes[track_id][: ,0])\n",
    "    tracks[track_id] = np.insert(tracks[track_id], len(tracks[track_id][0]), mask, axis=1)\n",
    "    track_id += 1\n",
    "\n",
    "with open(data_path + 'tracks_features_gaps_marked.npy', 'wb') as w:\n",
    "    np.save(w, np.array(tracks, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(data_path + 'tracks_features_gaps_marked.npy', allow_pickle=True)\n",
    "trackTargets = np.load(data_path + 'raw_trackTargets.npy', allow_pickle=True)\n",
    "tracksTrialId = np.load(data_path + 'raw_tracksTrialId.npy', allow_pickle=True)  \n",
    "\n",
    "tracks, trackTargets, tracksTrialId, trackGroup = split.split_tracks(tracks, trackTargets, tracksTrialId, segment_size, segment_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'tracks_split.npy', 'wb') as w:\n",
    "    np.save(w, tracks)\n",
    "with open(data_path + 'trackTargets_split.npy', 'wb') as w:\n",
    "    np.save(w, trackTargets)\n",
    "with open(data_path + 'trackGroup_split.npy', 'wb') as w:\n",
    "    np.save(w, trackGroup)\n",
    "with open(data_path + 'tracksTrialId_split.npy', 'wb') as w:\n",
    "    np.save(w, tracksTrialId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(data_path + 'tracks_split.npy', allow_pickle=True)\n",
    "trackTargets = np.load(data_path + 'trackTargets_split.npy', allow_pickle=True)\n",
    "trackGroup = np.load(data_path + 'trackGroup_split.npy', allow_pickle=True)  \n",
    "tracksTrialId = np.load(data_path + 'tracksTrialId_split.npy', allow_pickle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'X Velocity',\n",
    "    'Y Velocity',\n",
    "    'X Acceleration', \n",
    "    'Y Acceleration',\n",
    "    'Velocity',\n",
    "    'Acceleration',\n",
    "    'Jerk',\n",
    "    'Angular Velocity',\n",
    "    'Angular Acceleration',\n",
    "    'Angle of Flight',\n",
    "    'Centroid Distance Function',\n",
    "    'Persistence Velocity',\n",
    "    'Turning Velocity'\n",
    "]   \n",
    "indexes = [12,13,14,15,3,10,17,4,11,18,19,20,21]\n",
    "feature_stats = [\n",
    "    'mean','median','std', '1st quartile','3rd quartile','kurtosis', 'skewness',\n",
    "    'number of local minima','number of local maxima','number of zero-crossings']     \n",
    "\n",
    "track_statistics = dict()\n",
    "\n",
    "for col in feature_columns:\n",
    "    for stat in feature_stats:\n",
    "        track_statistics[f'{col} ({stat})'] = []\n",
    "\n",
    "for track in tracks:\n",
    "    data = extract.track_stats(track, indexes=indexes, columns=feature_columns)\n",
    "    for d in data:\n",
    "        track_statistics[d].append(data[d])\n",
    "\n",
    "df = pd.DataFrame(data=track_statistics)\n",
    "to_add = extract.add_other_features(tracks, (0,1))\n",
    "df = pd.concat([df, to_add], axis=1)\n",
    "\n",
    "df = df.join(pd.DataFrame({'TrialID': tracksTrialId}))\n",
    "\n",
    "df_target = pd.DataFrame({\n",
    "    'Target': trackTargets, 'TrialID': tracksTrialId, 'TrackGroup': trackGroup})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(data_path + 'df_raw.pkl')\n",
    "df_target.to_pickle(data_path + 'df_target_raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = df[df.isna().any(axis=1)].index\n",
    "df = df.drop(index=indexes)\n",
    "df_target = df_target.drop(index=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(data_path + 'df_clean.pkl')\n",
    "df_target.to_pickle(data_path + 'df_target_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Segment Quality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_path + 'df_raw.pkl')\n",
    "df_target = pd.read_pickle(data_path + 'df_target_raw.pkl')\n",
    "#tracks = np.load(data_path + 'tracks_split.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty_function(segment, n, m):\n",
    "    penalty_score = 0\n",
    "    k = 0\n",
    "\n",
    "    for position in segment:\n",
    "        if position == 0:\n",
    "            penalty_score += n * (m ** k)\n",
    "            k += 1\n",
    "        else:\n",
    "            k = max(0, k-1)\n",
    "\n",
    "    return penalty_score/len(segment)\n",
    "\n",
    "scores = []\n",
    "for segment in tracks:\n",
    "    mask = segment[:, -1]\n",
    "    scores.append(penalty_function(mask, n=1, m=1.05))\n",
    "scores = np.array(scores)\n",
    "joblib.dump(scores, data_path+'scores.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = joblib.load(data_path+'scores.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using maximum mutual information to obtain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_score_threshold_mutual_info(df, df_target, scores):\n",
    "    score_thresholds = np.linspace(0, max(scores), 250)\n",
    "    max_mutual_info_dict = {}\n",
    "    unique_features = []\n",
    "    for threshold in tqdm.tqdm(score_thresholds):\n",
    "        mask = np.where(scores <= threshold)[0]\n",
    "        df_temp = df.iloc[mask]\n",
    "        df_target_temp = df_target.iloc[mask]\n",
    "\n",
    "        indexes = df_temp[df_temp.isna().any(axis=1)].index\n",
    "        df_temp = df_temp.drop(index=indexes)\n",
    "        df_target_temp = df_target_temp.drop(index=indexes)\n",
    "\n",
    "        df_temp = extract.remove_nans(df_temp)        \n",
    "\n",
    "        df_temp = df_temp.drop(columns=['TrialID'])\n",
    "        df_target_temp = df_target_temp['Target']\n",
    "\n",
    "        mutual_info_values = mutual_info_classif(df_temp, df_target_temp)\n",
    "\n",
    "        unique_features += df_temp.columns.values.tolist()\n",
    "\n",
    "        for feature, mutual_info_value in zip(df_temp.columns, mutual_info_values):\n",
    "            if feature not in max_mutual_info_dict or max_mutual_info_dict[feature]['mutual_info'] < mutual_info_value:\n",
    "                max_mutual_info_dict[feature] = {\n",
    "                    'mutual_info': mutual_info_value,\n",
    "                    'score_threshold': threshold\n",
    "                }\n",
    "             \n",
    "            elif max_mutual_info_dict[feature]['mutual_info'] == mutual_info_value and max_mutual_info_dict[feature]['score_threshold'] < threshold:\n",
    "                max_mutual_info_dict[feature] = {\n",
    "                    'mutual_info': mutual_info_value,\n",
    "                    'score_threshold': threshold\n",
    "                }\n",
    "\n",
    "    unique_features = list(set(unique_features))\n",
    "    corresponding_values = [max_mutual_info_dict[feature]['score_threshold'] for feature in unique_features]\n",
    "\n",
    "    max_values = [max_mutual_info_dict[feature]['mutual_info'] for feature in unique_features]\n",
    "    exp = np.exp(np.array(max_values)/(np.array(corresponding_values)+1))\n",
    "    weights = exp / np.sum(exp)\n",
    "\n",
    "    weighted_average_threshold = np.average(corresponding_values, weights=weights)\n",
    "\n",
    "    return weighted_average_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = run_score_threshold_mutual_info(\n",
    "    df[df['TrialID'].isin(hyp_trials)],\n",
    "    df_target[df_target['TrialID'].isin(hyp_trials)], \n",
    "    scores[df_target['TrialID'].isin(hyp_trials)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.where(scores <= score_threshold)[0]\n",
    "df = df.iloc[mask]\n",
    "df_target = df_target.iloc[mask]\n",
    "\n",
    "indexes = df[df.isna().any(axis=1)].index\n",
    "df = df.drop(index=indexes)\n",
    "df_target = df_target.drop(index=indexes)\n",
    "\n",
    "df = extract.remove_nans(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(data_path + 'df_filtered.pkl')\n",
    "df_target.to_pickle(data_path + 'df_target_filtered.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using gradient of mutual information curves to obtain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(data, window_size):\n",
    "    gradients = np.gradient(data)\n",
    "    smoothed_gradients = np.convolve(gradients, np.ones(window_size)/window_size, mode='valid')\n",
    "    return smoothed_gradients\n",
    "\n",
    "def find_gradient(data, threshold=0.1, window_size=3):\n",
    "    gradients = calculate_gradient(data, window_size)\n",
    "    total_change = data[-1] - data[0]\n",
    "    threshold_value = threshold * total_change\n",
    "    for i, gradient in enumerate(gradients):\n",
    "        if gradient < threshold_value:\n",
    "            return i + window_size // 2 \n",
    "    return i\n",
    "\n",
    "def run_score_threshold_gradient(df, df_target, scores):\n",
    "    score_thresholds = np.linspace(0, max(scores), 250)\n",
    "\n",
    "    unique_features = []\n",
    "    all_scores_xls = []\n",
    "    for threshold in score_thresholds:\n",
    "        mask = np.where(scores <= threshold)[0]\n",
    "        df_temp = df.iloc[mask]\n",
    "        df_target_temp = df_target.iloc[mask]\n",
    "\n",
    "        indexes = df_temp[df_temp.isna().any(axis=1)].index\n",
    "        df_temp = df_temp.drop(index=indexes)\n",
    "        df_target_temp = df_target_temp.drop(index=indexes)\n",
    "\n",
    "        df_temp = extract.remove_nans(df_temp)        \n",
    "\n",
    "        df_temp = df_temp.drop(columns=['TrialID'])\n",
    "        df_target_temp = df_target_temp['Target']\n",
    "\n",
    "        mutual_info_values = mutual_info_classif(df_temp, df_target_temp)\n",
    "\n",
    "        unique_features += df_temp.columns.values.tolist()\n",
    "\n",
    "        for feature, mutual_info_value in zip(df_temp.columns.values, mutual_info_values):\n",
    "            all_scores_xls.append({'feature': feature, 'mutual_info': mutual_info_value, 'threshold': threshold})\n",
    "            \n",
    "    d = pd.DataFrame(all_scores_xls)\n",
    "    d = d.pivot(index='threshold', columns='feature', values='mutual_info')\n",
    "    d.reset_index(inplace=True)\n",
    "\n",
    "    feature_thresholds = {}\n",
    "    for feature in d.columns.values:\n",
    "        if feature != 'threshold':\n",
    "            index = find_gradient(d[feature].values)\n",
    "            feature_thresholds[feature] = {'threshold': d['threshold'].iloc[index], 'mutual_info': d[feature].iloc[index]}\n",
    "\n",
    "    unique_features = list(set(unique_features))\n",
    "    corresponding_values = [feature_thresholds[feature]['threshold'] for feature in unique_features]\n",
    "\n",
    "    max_values = [feature_thresholds[feature]['mutual_info'] for feature in unique_features]\n",
    "    exp = np.exp(max_values)\n",
    "    weights = exp / np.sum(exp)\n",
    "\n",
    "    weighted_average_threshold = np.average(corresponding_values, weights=weights)\n",
    "\n",
    "    return weighted_average_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = run_score_threshold_gradient(    \n",
    "    df[df['TrialID'].isin(hyp_trials)],\n",
    "    df_target[df_target['TrialID'].isin(hyp_trials)], \n",
    "    scores[df_target['TrialID'].isin(hyp_trials)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.where(scores <= score_threshold)[0]\n",
    "df = df.iloc[mask]\n",
    "df_target = df_target.iloc[mask]\n",
    "\n",
    "indexes = df[df.isna().any(axis=1)].index\n",
    "df = df.drop(index=indexes)\n",
    "df_target = df_target.drop(index=indexes)\n",
    "\n",
    "df = extract.remove_nans(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(data_path + 'df_filtered.pkl')\n",
    "df_target.to_pickle(data_path + 'df_target_filtered.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Train-Test/Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_path + 'df_filtered.pkl')\n",
    "df_target = pd.read_pickle(data_path + 'df_target_filtered.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['TrialID'].isin(test_trials)]\n",
    "df_train_target = df_target[df_target['TrialID'].isin(test_trials)]\n",
    "\n",
    "df_hyp = df[df['TrialID'].isin(hyp_trials)]\n",
    "df_hyp_target = df_target[df_target['TrialID'].isin(hyp_trials)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyp.to_pickle(data_path + 'df_hyp.pkl')\n",
    "df_hyp_target.to_pickle(data_path + 'df_hyp_target.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyp = df_hyp.drop(columns=['TrialID'])\n",
    "\n",
    "sus = df_hyp[df_hyp_target['Target'] == 0]\n",
    "res = df_hyp[df_hyp_target['Target'] == 1]\n",
    "_, p_val = mannwhitneyu(sus, res)\n",
    "rej, p_vals_corrected, _, _ = multipletests(p_val, alpha=0.05, method='holm')\n",
    "\n",
    "columns = df_hyp.columns[rej]\n",
    "\n",
    "df_hyp = df_hyp.reset_index(drop=True)\n",
    "corr_matrix = df_hyp.corr(method='spearman').abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "\n",
    "cols = np.setdiff1d(columns, to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for feat in cols:\n",
    "    if 'TrialID' not in feat:\n",
    "        features.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(results_path + 'features.txt', 'w+')\n",
    "features = []\n",
    "for feat in cols:\n",
    "    if 'TrialID' not in feat:\n",
    "        features.append(feat)\n",
    "        file.write(feat+'\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = open(results_path + 'features.txt', 'r+').read().split('\\n')\n",
    "features.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(features):\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds():\n",
    "    final_folds = []\n",
    "    for v1 in list(itertools.product([2,3], [15,16])):\n",
    "        for v2 in list(itertools.product([6,7,8], [11,12])):\n",
    "            train_trials = list(v1) + list(v2)\n",
    "            final_folds.append(train_trials)\n",
    "    return final_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = create_folds()\n",
    "file = open(results_path+'folds.txt', 'w+')\n",
    "for f in a:\n",
    "    file.write(str(f) +'\\n')\n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "\n",
    "def get_track_prediction(y_true, scores, preds, groups):\n",
    "    unique_groups = groups.unique()\n",
    "    track_preds = []\n",
    "    track_true = []\n",
    "    avg_scores = []\n",
    "    for val in unique_groups:\n",
    "        indexes = np.where(groups == val)[0]\n",
    "        track_true.append(mode(y_true.values[indexes]))\n",
    "        avg_scores.append(np.mean(scores[indexes]))\n",
    "        if np.mean(preds[indexes]) >= 0.5: \n",
    "            track_preds.append(1)\n",
    "        else:\n",
    "            track_preds.append(0)\n",
    "\n",
    "    return track_true, track_preds, avg_scores\n",
    "\n",
    "\n",
    "def produce_report(y_test, y_pred, scores):\n",
    "    precision_ir, recall_ir, _ = metrics.precision_recall_curve(y_test, scores)\n",
    "    precision_is, recall_is, _ = metrics.precision_recall_curve((np.array(y_test)*-1)+1 , 1-np.array(scores))\n",
    "    \n",
    "    TP, FP, TN, FN = perf_measure(y_test, y_pred)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, scores)\n",
    "    data = {\n",
    "        'accuracy': metrics.accuracy_score(y_test,y_pred),\n",
    "        'balanced accuracy': metrics.balanced_accuracy_score(y_test,y_pred),\n",
    "        'roc auc': metrics.roc_auc_score(y_test, scores),\n",
    "\n",
    "        'f1 score (ir)': metrics.f1_score(y_test,y_pred),\n",
    "        'precision (ir)': metrics.precision_score(y_test, y_pred),\n",
    "        'recall (ir)': metrics.recall_score(y_test, y_pred),\n",
    "        'pr auc (ir)': metrics.auc(recall_ir, precision_ir),\n",
    "\n",
    "        'f1 score (is)': metrics.f1_score(np.array(y_test)*-1 +1, np.array(y_pred)*-1 +1),\n",
    "        'precision (is)': metrics.precision_score(np.array(y_test)*-1 +1, np.array(y_pred)*-1 +1),\n",
    "        'recall (is)': metrics.recall_score(np.array(y_test)*-1 +1, np.array(y_pred)*-1 +1),\n",
    "        'pr auc (is)': metrics.auc(recall_is, precision_is),\n",
    "\n",
    "        'cohen kappa': metrics.cohen_kappa_score(np.array(y_test), np.array(y_pred)),\n",
    "        'matthews correlation coefficient': metrics.matthews_corrcoef(np.array(y_test), np.array(y_pred)),\n",
    "        'log loss': metrics.log_loss(np.array(y_test), np.array(scores)),\n",
    "        'tp': TP, 'tn': TN, 'fp': FP, 'fn': FN,\n",
    "        'tpr': tpr, 'fpr': fpr\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_model(x_train, y_train, x_test, y_test):\n",
    "    model = DummyRegressor(strategy='constant', constant=0)\n",
    "    model.fit(x_train, y_train['Target'])\n",
    "    y_pred = model.predict(x_test)\n",
    "    track_true, track_preds, scores = get_track_prediction(y_test['Target'], y_pred, y_pred, y_test['TrackGroup'])\n",
    "    return produce_report(track_true, track_preds, scores)  , {\n",
    "        'segment-preds': y_pred, \n",
    "        'segment-target': y_test['Target']\n",
    "    }  \n",
    "\n",
    "def logistic_model(x_train, y_train, x_test, y_test, params):\n",
    "    model = LogisticRegression(\n",
    "        random_state=0,\n",
    "        **params\n",
    "\n",
    "    )\n",
    "    model.fit(x_train, y_train['Target'])\n",
    "    scores = model.predict_proba(x_test)[:,1]\n",
    "    y_pred = model.predict(x_test)\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(y_test['Target'], scores, y_pred, y_test['TrackGroup'])\n",
    "    return produce_report(track_true, track_preds, avg_scores), {\n",
    "        'segment-preds': y_pred, \n",
    "        'segment-target': y_test['Target']\n",
    "    }, model\n",
    "\n",
    "def random_forest_model(x_train, y_train, x_test, y_test, params):\n",
    "    model = RandomForestClassifier(\n",
    "        **params\n",
    "    )\n",
    "    model.fit(x_train, y_train['Target'])\n",
    "    y_pred = model.predict(x_test)\n",
    "    scores = model.predict_proba(x_test)[:,1]\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(y_test['Target'], scores, y_pred, y_test['TrackGroup'])\n",
    "    return produce_report(track_true, track_preds, avg_scores), {\n",
    "        'segment-preds': y_pred, \n",
    "        'segment-target': y_test['Target']\n",
    "    }, model\n",
    "\n",
    "def xgboost_model(x_train, y_train, x_test, y_test, params):\n",
    "    model = XGBClassifier(\n",
    "        **params\n",
    "    )\n",
    "    model.fit(x_train, y_train['Target'])\n",
    "    y_pred = model.predict(x_test)\n",
    "    scores = model.predict_proba(x_test)[:,1]\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(y_test['Target'], scores, y_pred, y_test['TrackGroup'])\n",
    "    return produce_report(track_true, track_preds, avg_scores), {\n",
    "        'segment-preds': y_pred, \n",
    "        'segment-target': y_test['Target']\n",
    "    }, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict(\n",
    "    dummy_results_train = [],\n",
    "    dummy_results_test = [],\n",
    "    logistic_results_train = [],\n",
    "    logistic_results_test = [],\n",
    "    random_forests_train = [],\n",
    "    random_forests_test = [],\n",
    "    xgboost_train = [],\n",
    "    xgboost_test = [],\n",
    ")\n",
    "segment_scores_train = []\n",
    "segment_scores_test = []\n",
    "\n",
    "segment_scores_lr_train = []\n",
    "segment_scores_lr_test = []\n",
    "\n",
    "segment_scores_constant_train = []\n",
    "segment_scores_constant_test = []\n",
    "\n",
    "segment_scores_xgboost_train = []\n",
    "segment_scores_xgboost_test = []\n",
    "\n",
    "folds = create_folds()\n",
    "\n",
    "\n",
    "for index, fold in enumerate(folds):\n",
    "    print(f' --- FOLD {index} ---')\n",
    "    train_trials = fold\n",
    "    mask = df_train_target['TrialID'].isin(train_trials)\n",
    "\n",
    "    train = df_train[mask]\n",
    "    train_targets = df_train_target[mask]\n",
    "    \n",
    "    test = df_train[~mask]\n",
    "    test_targets = df_train_target[~mask]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train = pd.DataFrame(scaler.fit_transform(train[features]), columns=features, index=train.index)\n",
    "    test = pd.DataFrame(scaler.transform(test[features]), columns=features, index=test.index)\n",
    "\n",
    "    sm = SMOTE(\n",
    "        random_state=0\n",
    "    )\n",
    "    train_os, train_targets_os = sm.fit_resample(train, train_targets.drop(columns=['TrialID','TrackGroup']))\n",
    "    \n",
    "    constant_scores, constant_segment_scores = constant_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=train, \n",
    "        y_test=train_targets)\n",
    "    results['dummy_results_train'].append(constant_scores)\n",
    "    segment_scores_constant_train.append(constant_segment_scores)\n",
    "\n",
    "    constant_scores, constant_segment_scores = constant_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=test, \n",
    "        y_test=test_targets)\n",
    "\n",
    "    results['dummy_results_test'].append(constant_scores)\n",
    "    segment_scores_constant_test.append(constant_segment_scores)\n",
    "    \n",
    "    \n",
    "    lr_scores, lr_segment_scores, model = logistic_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=train, \n",
    "        y_test=train_targets,\n",
    "        params=dict(\n",
    "            penalty='l2',\n",
    "            C=0.01,\n",
    "            solver='saga',\n",
    "            max_iter=50\n",
    "        )\n",
    "    )\n",
    "    results['logistic_results_train'].append(lr_scores)\n",
    "    segment_scores_lr_train.append(lr_segment_scores)\n",
    "    \n",
    "    lr_scores, lr_segment_scores, model = logistic_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=test, \n",
    "        y_test=test_targets,\n",
    "        params=dict(\n",
    "            penalty='l2',\n",
    "            C=0.01,\n",
    "            solver='saga',\n",
    "            max_iter=50\n",
    "        )\n",
    "    )\n",
    "    results['logistic_results_test'].append(lr_scores)\n",
    "    segment_scores_lr_test.append(lr_segment_scores)\n",
    "    joblib.dump(dict(\n",
    "        model=model,\n",
    "        df_train=df_train,\n",
    "        df_train_target=df_train_target,\n",
    "        features=features,\n",
    "        train_os=train_os,\n",
    "        test=test,\n",
    "        mask=mask,\n",
    "    ), data_path+f'shap/logistic_shap_dump_{index}.dat')\n",
    "    \n",
    "     \n",
    "    rf_scores, segment_scores, model = random_forest_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=train, \n",
    "        y_test=train_targets,\n",
    "        params=dict(\n",
    "            random_state=0,\n",
    "            n_estimators=300,\n",
    "            criterion='entropy',\n",
    "            max_depth=15,\n",
    "            min_samples_split=3,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='sqrt',\n",
    "            bootstrap=False,\n",
    "            n_jobs=8\n",
    "        ))\n",
    "    results['random_forests_train'].append(rf_scores)\n",
    "    segment_scores_train.append(segment_scores)\n",
    "\n",
    "    rf_scores, segment_scores, model = random_forest_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=test, \n",
    "        y_test=test_targets,\n",
    "        params=dict(\n",
    "            random_state=0,\n",
    "            n_estimators=300,\n",
    "            criterion='entropy',\n",
    "            max_depth=15,\n",
    "            min_samples_split=3,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='sqrt',\n",
    "            bootstrap=False,\n",
    "            n_jobs=8\n",
    "        ))\n",
    "    results['random_forests_test'].append(rf_scores)\n",
    "    segment_scores_test.append(segment_scores)\n",
    "    joblib.dump(dict(\n",
    "        model=model,\n",
    "        df_train=df_train,\n",
    "        df_train_target=df_train_target,\n",
    "        features=features,\n",
    "        test=test,\n",
    "        mask=mask,\n",
    "        train_os=train_os,\n",
    "    ), data_path+f'shap/random_forests_shap_dump_{index}.dat')\n",
    "     \n",
    "    \n",
    "    xg_scores, segment_scores, model = xgboost_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=train, \n",
    "        y_test=train_targets,\n",
    "        params=dict(\n",
    "            random_state=0,\n",
    "            learning_rate=0.3,\n",
    "            n_estimators=250,\n",
    "            max_depth=5,\n",
    "            subsample=0.5,\n",
    "            colsample_bytree=0.9,\n",
    "            reg_alpha=0, \n",
    "            reg_lambda=0,\n",
    "            min_child_weight=15\n",
    "        ))\n",
    "    results['xgboost_train'].append(xg_scores)\n",
    "    segment_scores_xgboost_train.append(segment_scores)\n",
    "\n",
    "    xg_scores, segment_scores, model = xgboost_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=test, \n",
    "        y_test=test_targets,\n",
    "        params=dict(\n",
    "            random_state=0,\n",
    "            learning_rate=0.3,\n",
    "            n_estimators=250,\n",
    "            max_depth=5,\n",
    "            subsample=0.5,\n",
    "            colsample_bytree=0.9,\n",
    "            reg_alpha=0, \n",
    "            reg_lambda=0,\n",
    "            min_child_weight=15\n",
    "        ))\n",
    "    results['xgboost_test'].append(xg_scores)\n",
    "    segment_scores_xgboost_test.append(segment_scores)\n",
    "    joblib.dump(dict(\n",
    "        model=model,\n",
    "        df_train=df_train,\n",
    "        df_train_target=df_train_target,\n",
    "        features=features,\n",
    "        test=test,\n",
    "        test_targets=test_targets,\n",
    "        mask=mask,\n",
    "        train_os=train_os,\n",
    "    ), data_path+f'shap-1/xgboost_shap_dump_{index}.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_path+'results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_path+'results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.create_sheet()\n",
    "\n",
    "row = 2\n",
    "metrics_list = list(results['random_forests_train'][0].keys())\n",
    "for i, column in enumerate(['model'] + metrics_list):\n",
    "    sheet.cell(row=1, column=i+1).value = column\n",
    "\n",
    "for model in ['dummy_results', 'logistic_results', 'random_forests', 'xgboost']:\n",
    "    for model_type in ['train', 'test']:\n",
    "        try:\n",
    "            sheet.cell(row=row, column=1).value = f'{model.upper()} {model_type.upper()}'\n",
    "            for j, metric in enumerate(results[model+'_'+model_type][0].keys()):\n",
    "                if metric in ['tp', 'tn', 'fp', 'fn']:\n",
    "                    scores = 0\n",
    "                    for fold in range(len(results[model+'_'+model_type])):\n",
    "                        scores += results[model+'_'+model_type][fold][metric]\n",
    "                    \n",
    "                    sheet.cell(row=row, column=j+2).value = scores \n",
    "                elif metric in ['tpr', 'fpr']:\n",
    "                    pass\n",
    "                else:\n",
    "                    scores = []\n",
    "                    for fold in range(len(results[model+'_'+model_type])):\n",
    "                        scores.append(results[model+'_'+model_type][fold][metric])\n",
    "                    \n",
    "                    sheet.cell(row=row, column=j+2).value = f'{round(np.mean(scores), 3)} ({round(min(scores), 3)} - {round(max(scores), 3)})'\n",
    "            row += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "wb.save(results_path + 'scores.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONFUSION MATRIX'''\n",
    "\n",
    "def plot_confusion_matrix(tp, tn, fp, fn, classifier):\n",
    "    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    im = ax.imshow(confusion_matrix, cmap='Oranges')\n",
    "    ax.set_xlabel('Predicted Class')\n",
    "    ax.set_ylabel('True Class')\n",
    "    #ax.set_title(f'Confusion Matrix - {classifier}')\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['IS', 'IR'])\n",
    "    ax.set_yticklabels(['IS', 'IR'])\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, confusion_matrix[i, j], ha='center', va='center', fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix_percent(tp, tn, fp, fn, classifier):\n",
    "    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    im = ax.imshow(confusion_matrix, cmap='Oranges')\n",
    "    ax.set_xlabel('Predicted Class')\n",
    "    ax.set_ylabel('True Class')\n",
    "    #ax.set_title(f'Confusion Matrix - {classifier}')\n",
    "\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['IS', 'IR'])\n",
    "    ax.set_yticklabels(['IS', 'IR'])\n",
    "\n",
    "    total_samples = np.sum(confusion_matrix)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            percentage = confusion_matrix[i, j] / total_samples * 100\n",
    "            ax.text(j, i, f'{percentage:.2f}%', ha='center', va='center', color='black', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for key in ['dummy_results_train', 'dummy_results_test','logistic_results_train','logistic_results_test','random_forests_train','random_forests_test', 'xgboost_train', 'xgboost_test']:\n",
    "    print(key)\n",
    "    plot_confusion_matrix_percent(\n",
    "        sum([_['tp'] for _ in results[key]]), \n",
    "        sum([_['tn'] for _ in results[key]]), \n",
    "        sum([_['fp'] for _ in results[key]]), \n",
    "        sum([_['fn'] for _ in results[key]]),\n",
    "        classifier=key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ROC CURVES'''\n",
    "\n",
    "def plot_roc_curve(tpr, fpr, classifier):\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    for i in range(len(tpr)):\n",
    "        ax.plot(fpr[i], tpr[i], alpha=0.5)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    #ax.set_title(f'ROC Curves - {classifier}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for key in ['dummy_results_train', 'dummy_results_test','logistic_results_train','logistic_results_test','random_forests_train','random_forests_test', 'xgboost_train', 'xgboost_test']:\n",
    "    print(key)\n",
    "    plot_roc_curve(\n",
    "        [_['tpr'] for _ in results[key]],  \n",
    "        [_['fpr'] for _ in results[key]], \n",
    "        classifier=key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PR CURVES'''\n",
    "\n",
    "def plot_pr_curve(precision, recall, classifier):\n",
    "    fig, ax = plt.subplots()\n",
    "    base = np.linspace(0, 1, 101)\n",
    "    avg_precision = 1\n",
    "\n",
    "    for i in range(len(precision)):\n",
    "        ax.plot(recall[i], precision[i])\n",
    "        new_avg_precision = sum(precision[i]) / len(precision[i])\n",
    "        if new_avg_precision < avg_precision:\n",
    "            avg_precision = new_avg_precision\n",
    "\n",
    "    ax.plot(base, [avg_precision]*len(base), linestyle='--')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title(f'PR Curves - {classifier}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for key in ['dummy_results_train', 'dummy_results_test','logistic_results_train','logistic_results_test','random_forests_train','random_forests_test', 'xgboost_train', 'xgboost_test']:\n",
    "    plot_pr_curve(\n",
    "        [results[key][i]['precision (ir) (curve)'] for i in range(len(results[key]))],\n",
    "        [results[key][i]['recall (ir) (curve)'] for i in range(len(results[key]))], \n",
    "        classifier=key+' (ir)'\n",
    "    )\n",
    "    plot_pr_curve(\n",
    "        [results[key][i]['precision (is) (curve)'] for i in range(len(results[key]))],\n",
    "        [results[key][i]['recall (is) (curve)'] for i in range(len(results[key]))], \n",
    "        classifier=key+' (is)'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''EXCEL FILE OF ALL FOLD SCORES'''\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "for key in ['dummy_results_train', 'dummy_results_test','logistic_results_train','logistic_results_test','random_forests_train','random_forests_test', 'xgboost_train', 'xgboost_test']:\n",
    "    sheet = wb.create_sheet(key.upper())\n",
    "    columns = ['fold', 'test trials', 'train trials', 'accuracy', 'balanced accuracy',\n",
    "    'f1 score (ir)', 'roc auc', 'precision (ir)', 'recall (ir)', 'pr auc (ir)',\n",
    "    'f1 score (is)', 'precision (is)', 'recall (is)', 'pr auc (is)',\n",
    "    'cohen kappa', 'matthews correlation coefficient', 'log loss', 'tp', 'tn', 'fp', 'fn']\n",
    "    for i, column in enumerate(columns):\n",
    "        sheet.cell(row=1, column=i+1).value = column\n",
    "\n",
    "        for row in range(len(results[key])):\n",
    "            if column not in ['fold', 'test trials', 'train trials']:\n",
    "                sheet.cell(row=row+2, column=i+1).value = results[key][row][column]\n",
    "        \n",
    "            elif column == 'fold':\n",
    "                sheet.cell(row=row+2, column=i+1).value = row\n",
    "\n",
    "            elif column == 'test trials':\n",
    "                train = folds[row]\n",
    "                all_ids = df_train_target['TrialID'].unique()\n",
    "                sheet.cell(row=row+2, column=i+1).value = str([x for x in all_ids if x not in train])\n",
    "\n",
    "            elif column == 'train trials':\n",
    "                sheet.cell(row=row+2, column=i+1).value = str(folds[row])\n",
    "\n",
    "wb.save(results_path + \"all-folds.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94d2f0a72470cbfa5bf040eb69ac68719f2f2aa4e7158c9baf86afd24ebc134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
