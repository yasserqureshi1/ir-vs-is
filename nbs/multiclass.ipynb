{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from statistics import mode\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import openpyxl\n",
    "\n",
    "import sys \n",
    "import joblib\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "sys.path.append('H:/Documents/PhD/3rd-year-project/classify-mosquitoes/src/')\n",
    "import extract, split, config\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment Size and Overlap (in seconds)\n",
    "segment_size = 6\n",
    "segment_overlap = 5.5\n",
    "\n",
    "# Trials split between test/train and validation set\n",
    "test_trials = np.array([2,3 ,6,7,8, 11,12, 15,16])\n",
    "target_trials = np.array([0,0, 1,1,1, 2,2, 3,3])\n",
    "hyp_trials = np.array([0,1, 4,5, 9,10, 13,14])\n",
    "\n",
    "# Paths\n",
    "results_path = config.PATH + 'tuned model/multiclass/' # Results stored\n",
    "data_path = results_path + 'data/' # Any data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks, trackTargets, tracksTrialId = extract.load(config.FILE, config.PATH, config.IS_RESISTANT, config.DATA_PATH)\n",
    "\n",
    "with open(data_path + 'raw_tracks.npy', 'wb') as w:\n",
    "    np.save(w, np.array(tracks, dtype=object))\n",
    "with open(data_path + 'raw_trackTargets.npy', 'wb') as w:\n",
    "    np.save(w, np.array(trackTargets, dtype=object))\n",
    "with open(data_path + 'raw_tracksTrialId.npy', 'wb') as w:\n",
    "    np.save(w, np.array(tracksTrialId, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(data_path + 'raw_tracks.npy', allow_pickle=True)\n",
    "trackTargets = np.load(data_path + 'raw_trackTargets.npy', allow_pickle=True)\n",
    "tracksTrialId = np.load(data_path + 'raw_tracksTrialId.npy', allow_pickle=True)   \n",
    "\n",
    "tracks = extract.generate_features(tracks, (0,1), 2)\n",
    "\n",
    "with open(data_path + 'tracks_features.npy', 'wb') as w:\n",
    "    np.save(w, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(data_path + 'tracks_features.npy', allow_pickle=True)\n",
    "bmodes = joblib.load('bmodess.dat')\n",
    "track_id = 0\n",
    "while track_id < len(tracks):\n",
    "    mask = np.isin(tracks[track_id][:, 16], bmodes[track_id][: ,0])\n",
    "    tracks[track_id] = np.insert(tracks[track_id], len(tracks[track_id][0]), mask, axis=1)\n",
    "    track_id += 1\n",
    "\n",
    "with open(data_path + 'tracks_features_gaps_marked.npy', 'wb') as w:\n",
    "    np.save(w, np.array(tracks, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = np.load(data_path + 'tracks_features_gaps_marked.npy', allow_pickle=True)\n",
    "trackTargets = np.load(data_path + 'raw_trackTargets.npy', allow_pickle=True)\n",
    "tracksTrialId = np.load(data_path + 'raw_tracksTrialId.npy', allow_pickle=True)  \n",
    "\n",
    "tracks, trackTargets, tracksTrialId, trackGroup = split.split_tracks(tracks, trackTargets, tracksTrialId, segment_size, segment_overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(data_path + 'tracks_split.npy', 'wb') as w:\n",
    "    np.save(w, tracks)\n",
    "with open(data_path + 'trackTargets_split.npy', 'wb') as w:\n",
    "    np.save(w, trackTargets)\n",
    "with open(data_path + 'trackGroup_split.npy', 'wb') as w:\n",
    "    np.save(w, trackGroup)\n",
    "with open(data_path + 'tracksTrialId_split.npy', 'wb') as w:\n",
    "    np.save(w, tracksTrialId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'X Velocity',\n",
    "    'Y Velocity',\n",
    "    'X Acceleration', \n",
    "    'Y Acceleration',\n",
    "    'Velocity',\n",
    "    'Acceleration',\n",
    "    'Jerk',\n",
    "    'Angular Velocity',\n",
    "    'Angular Acceleration',\n",
    "    'Angle of Flight',\n",
    "    'Centroid Distance Function',\n",
    "    'Persistence Velocity',\n",
    "    'Turning Velocity'\n",
    "]   \n",
    "indexes = [12,13,14,15,3,10,17,4,11,18,19,20,21]\n",
    "feature_stats = ['mean','median','std', '1st quartile','3rd quartile','kurtosis', 'skewness','number of local minima','number of local maxima','number of zero-crossings']     \n",
    "\n",
    "track_statistics = dict()\n",
    "\n",
    "for col in feature_columns:\n",
    "    for stat in feature_stats:\n",
    "        track_statistics[f'{col} ({stat})'] = []\n",
    "\n",
    "for track in tracks:\n",
    "    data = extract.track_stats(track, indexes=indexes, columns=feature_columns)\n",
    "    for d in data:\n",
    "        track_statistics[d].append(data[d])\n",
    "\n",
    "df = pd.DataFrame(data=track_statistics)\n",
    "to_add = extract.add_other_features(tracks, (0,1))\n",
    "df = pd.concat([df, to_add], axis=1)\n",
    "\n",
    "df = df.join(pd.DataFrame({'TrialID': tracksTrialId}))\n",
    "\n",
    "df_target = pd.DataFrame({'Target': trackTargets, 'TrialID': tracksTrialId, 'TrackGroup': trackGroup})\n",
    "\n",
    "df.to_pickle(data_path + 'df.pkl')\n",
    "df_target.to_pickle(data_path + 'df_target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_path + 'df.pkl')\n",
    "df_target = pd.read_pickle(data_path + 'df_target.pkl')\n",
    "#tracks = np.load(data_path + 'tracks_split.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.loc[df_target['TrialID'].isin([0,1,2,3]), 'Target'] = 0\n",
    "df_target.loc[df_target['TrialID'].isin([4,5,6,7,8]), 'Target'] = 1\n",
    "df_target.loc[df_target['TrialID'].isin([9,10,11,12]), 'Target'] = 2\n",
    "df_target.loc[df_target['TrialID'].isin([13,14,15,16]), 'Target'] = 3\n",
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty_function(segment, n, m):\n",
    "    penalty_score = 0\n",
    "    k = 0\n",
    "\n",
    "    for position in segment:\n",
    "        if position == 0:\n",
    "            penalty_score += n * (m ** k)\n",
    "            k += 1\n",
    "        else:\n",
    "            k = max(0, k-1)\n",
    "\n",
    "    return penalty_score/len(segment)\n",
    "\n",
    "scores = []\n",
    "for segment in tracks:\n",
    "    mask = segment[:, -1]\n",
    "    scores.append(penalty_function(mask, n=1, m=1.05))\n",
    "scores = np.array(scores)\n",
    "joblib.dump(scores, data_path + 'scores.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = joblib.load(data_path + 'scores.dat')\n",
    "df = pd.read_pickle(data_path + 'df.pkl')\n",
    "df_target = pd.read_pickle(data_path + 'df_target.pkl')\n",
    "\n",
    "df_target.loc[df_target['TrialID'].isin([0,1,2,3]), 'Target'] = 0\n",
    "df_target.loc[df_target['TrialID'].isin([4,5,6,7,8]), 'Target'] = 1\n",
    "df_target.loc[df_target['TrialID'].isin([9,10,11,12]), 'Target'] = 2\n",
    "df_target.loc[df_target['TrialID'].isin([13,14,15,16]), 'Target'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_score_threshold_mutual_info(df, df_target, scores):\n",
    "    score_thresholds = np.linspace(0, max(scores), 250)\n",
    "    max_mutual_info_dict = {}\n",
    "    unique_features = []\n",
    "    for threshold in tqdm.tqdm(score_thresholds):\n",
    "        mask = np.where(scores <= threshold)[0]\n",
    "        df_temp = df.iloc[mask]\n",
    "        df_target_temp = df_target.iloc[mask]\n",
    "\n",
    "        indexes = df_temp[df_temp.isna().any(axis=1)].index\n",
    "        df_temp = df_temp.drop(index=indexes)\n",
    "        df_target_temp = df_target_temp.drop(index=indexes)\n",
    "\n",
    "        df_temp = extract.remove_nans(df_temp)        \n",
    "\n",
    "        df_temp = df_temp.drop(columns=['TrialID'])\n",
    "        df_target_temp = df_target_temp['Target']\n",
    "\n",
    "        mutual_info_values = mutual_info_classif(df_temp, df_target_temp)\n",
    "\n",
    "        unique_features += df_temp.columns.values.tolist()\n",
    "\n",
    "        for feature, mutual_info_value in zip(df_temp.columns, mutual_info_values):\n",
    "            if feature not in max_mutual_info_dict or max_mutual_info_dict[feature]['mutual_info'] < mutual_info_value:\n",
    "                max_mutual_info_dict[feature] = {\n",
    "                    'mutual_info': mutual_info_value,\n",
    "                    'score_threshold': threshold\n",
    "                }\n",
    "             \n",
    "            elif max_mutual_info_dict[feature]['mutual_info'] == mutual_info_value and max_mutual_info_dict[feature]['score_threshold'] < threshold:\n",
    "                max_mutual_info_dict[feature] = {\n",
    "                    'mutual_info': mutual_info_value,\n",
    "                    'score_threshold': threshold\n",
    "                }\n",
    "\n",
    "    unique_features = list(set(unique_features))\n",
    "    corresponding_values = [max_mutual_info_dict[feature]['score_threshold'] for feature in unique_features]\n",
    "\n",
    "    max_values = [max_mutual_info_dict[feature]['mutual_info'] for feature in unique_features]\n",
    "    exp = np.exp(np.array(max_values)/(np.array(corresponding_values)+1))\n",
    "    weights = exp / np.sum(exp)\n",
    "\n",
    "    weighted_average_threshold = np.average(corresponding_values, weights=weights)\n",
    "\n",
    "    return weighted_average_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = run_score_threshold_mutual_info(\n",
    "    df[df['TrialID'].isin(hyp_trials)],\n",
    "    df_target[df_target['TrialID'].isin(hyp_trials)], \n",
    "    scores[df_target['TrialID'].isin(hyp_trials)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.where(scores <= score_threshold)[0]\n",
    "df = df.iloc[mask]\n",
    "df_target = df_target.iloc[mask]\n",
    "\n",
    "indexes = df[df.isna().any(axis=1)].index\n",
    "df = df.drop(index=indexes)\n",
    "df_target = df_target.drop(index=indexes)\n",
    "\n",
    "df = extract.remove_nans(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(data_path + 'df_filtered.pkl')\n",
    "df_target.to_pickle(data_path + 'df_target_filtered.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Train-Test/Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(data_path + 'df_filtered.pkl')\n",
    "df_target = pd.read_pickle(data_path + 'df_target_filtered.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['TrialID'].isin(test_trials)]\n",
    "df_train_target = df_target[df_target['TrialID'].isin(test_trials)]\n",
    "\n",
    "df_hyp = df[df['TrialID'].isin(hyp_trials)]\n",
    "df_hyp_target = df_target[df_target['TrialID'].isin(hyp_trials)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyp = df_hyp.drop(columns=['TrialID'])\n",
    "\n",
    "df_0 = df_hyp.loc[df_hyp_target[df_hyp_target['Target'] == 0].index.values]\n",
    "df_1 = df_hyp.loc[df_hyp_target[df_hyp_target['Target'] == 1].index.values]\n",
    "df_2 = df_hyp.loc[df_hyp_target[df_hyp_target['Target'] == 2].index.values]\n",
    "df_3 = df_hyp.loc[df_hyp_target[df_hyp_target['Target'] == 3].index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(data0, data1):\n",
    "    _, p_val = mannwhitneyu(data0, data1)\n",
    "    rej, _, _, _ = multipletests(p_val, alpha=0.05, method='holm')\n",
    "    columns = df_hyp.columns[rej]\n",
    "    return list(columns)\n",
    "\n",
    "columns = []\n",
    "df_all = list(itertools.combinations([df_0, df_1, df_2, df_3], 2))\n",
    "for d in df_all:\n",
    "    d1, d2 = d\n",
    "    cols = feature_selection(d1, d2)\n",
    "    columns += cols\n",
    "\n",
    "columns = list(set(columns))\n",
    "\n",
    "df_hyp = df_hyp.reset_index()\n",
    "corr_matrix = df_hyp.corr(method='spearman').abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "\n",
    "cols = np.setdiff1d(columns, to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for feat in cols:\n",
    "    if 'TrialID' not in feat:\n",
    "        features.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(data_path + 'features.txt', 'w+')\n",
    "features = []\n",
    "for feat in cols:\n",
    "    if 'TrialID' not in feat:\n",
    "        features.append(feat)\n",
    "        file.write(feat+'\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = open(data_path + 'features.txt', 'r+').read().split('\\n')\n",
    "features.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds():\n",
    "    final_folds = []\n",
    "    for v1 in list(itertools.product([2,3], [15,16])):\n",
    "        for v2 in list(itertools.product([6,7,8], [11,12])):\n",
    "            train_trials = list(v1) + list(v2)\n",
    "            final_folds.append(train_trials)\n",
    "    return final_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = create_folds()\n",
    "file = open(data_path+'folds.txt', 'w+')\n",
    "for f in a:\n",
    "    file.write(str(f) +'\\n')\n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_prediction(y_true, scores, preds, groups, classes):\n",
    "    unique_groups = groups.unique()\n",
    "    track_preds = []\n",
    "    track_true = []\n",
    "    avg_scores = []\n",
    "    for val in unique_groups:\n",
    "        indexes = np.where(groups == val)[0]\n",
    "        track_true.append(mode(y_true.values[indexes]))\n",
    "        score = np.mean(scores[indexes], axis=0)\n",
    "        score_index = np.argmax(score)\n",
    "        avg_scores.append(score[score_index])\n",
    "        track_preds.append(classes[score_index])\n",
    "    return track_true, track_preds, avg_scores\n",
    "\n",
    "\n",
    "def produce_report(y_test, y_pred, scores):\n",
    "    report = metrics.classification_report(y_test, y_pred, output_dict=True)\n",
    "    data = {\n",
    "        'accuracy': metrics.accuracy_score(y_test,y_pred),\n",
    "        'balanced accuracy': metrics.balanced_accuracy_score(y_test,y_pred),\n",
    "        'report': report\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_model(x_train, y_train, x_test, y_test, params):\n",
    "    model = XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_classes=4,\n",
    "        **params\n",
    "    )\n",
    "    model.fit(x_train, y_train['Target'])\n",
    "    y_pred = model.predict(x_test)\n",
    "    scores = model.predict_proba(x_test)\n",
    "    track_true, track_preds, avg_scores = get_track_prediction(y_test['Target'], scores, y_pred, y_test['TrackGroup'], model.classes_)\n",
    "    return produce_report(track_true, track_preds, avg_scores), {\n",
    "        'track-preds': track_true, \n",
    "        'track-target': track_preds\n",
    "    }, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict(\n",
    "    xgboost_train = [],\n",
    "    xgboost_test = []\n",
    ")\n",
    "\n",
    "train_track_preds = []\n",
    "test_track_preds = []\n",
    "\n",
    "folds = create_folds()\n",
    "\n",
    "df_train_target['Target'] = df_train_target['Target'].astype(int)\n",
    "\n",
    "for index, fold in enumerate(folds):\n",
    "    print(f' --- FOLD {index} ---')\n",
    "    train_trials = fold\n",
    "    mask = df_train_target['TrialID'].isin(train_trials)\n",
    "\n",
    "    train = df_train[mask]\n",
    "    train_targets = df_train_target[mask]\n",
    "    \n",
    "    test = df_train[~mask]\n",
    "    test_targets = df_train_target[~mask]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train = scaler.fit_transform(train[features])\n",
    "    test = scaler.transform(test[features])\n",
    "\n",
    "    sm = SMOTE(\n",
    "        random_state=0\n",
    "    )\n",
    "    train_os, train_targets_os = sm.fit_resample(train, train_targets.drop(columns=['TrialID','TrackGroup']))\n",
    "    train_targets_os = train_targets_os.astype(int)\n",
    "\n",
    "    xg_scores, segment_scores, model = xgboost_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=train, \n",
    "        y_test=train_targets,\n",
    "        params=dict(\n",
    "            random_state=0,\n",
    "            learning_rate=0.3,\n",
    "            n_estimators=200,\n",
    "            max_depth=5,\n",
    "            subsample=0.5,\n",
    "            colsample_bytree=0.5,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            min_child_weight=5\n",
    "        ))\n",
    "    results['xgboost_train'].append(xg_scores)\n",
    "    train_track_preds.append(segment_scores)\n",
    "\n",
    "    xg_scores, segment_scores, model = xgboost_model(\n",
    "        x_train=train_os, \n",
    "        y_train=train_targets_os, \n",
    "        x_test=test, \n",
    "        y_test=test_targets,\n",
    "        params=dict(\n",
    "            random_state=0,\n",
    "            learning_rate=0.3,\n",
    "            n_estimators=200,\n",
    "            max_depth=5,\n",
    "            subsample=0.5,\n",
    "            colsample_bytree=0.5,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            min_child_weight=5\n",
    "        ))\n",
    "    results['xgboost_test'].append(xg_scores)\n",
    "    test_track_preds.append(segment_scores)\n",
    "    joblib.dump(dict(\n",
    "        model=model,\n",
    "        df_train=df_train,\n",
    "        df_train_target=df_train_target,\n",
    "        features=features,\n",
    "        test=test,\n",
    "        mask=mask,\n",
    "        train_os=train_os,\n",
    "    ), data_path+f'shap/xgboost_shap_dump_{index}.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_path+'train_track_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(train_track_preds, f)\n",
    "    \n",
    "with open(results_path+'results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "with open(results_path+'test_track_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(test_track_preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_0 = []\n",
    "acc_1 = []\n",
    "acc_2 = []\n",
    "acc_3 = []\n",
    "\n",
    "for s in test_track_preds:\n",
    "    preds = s['track-preds']\n",
    "    actual = s['track-target']#.values\n",
    "\n",
    "    indices = [i for i, x in enumerate(actual) if x == 0]\n",
    "    n_correct = sum(preds[i] == 0 for i in indices)\n",
    "    n_total = len(indices)\n",
    "    acc = n_correct / n_total\n",
    "    acc_0.append(acc)\n",
    "\n",
    "    indices = [i for i, x in enumerate(actual) if x == 1]\n",
    "    n_correct = sum(preds[i] == 1 for i in indices)\n",
    "    n_total = len(indices)\n",
    "    acc = n_correct / n_total\n",
    "    acc_1.append(acc)\n",
    "\n",
    "    indices = [i for i, x in enumerate(actual) if x == 2]\n",
    "    n_correct = sum(preds[i] == 2 for i in indices)\n",
    "    n_total = len(indices)\n",
    "    acc = n_correct / n_total\n",
    "    acc_2.append(acc)\n",
    "\n",
    "    indices = [i for i, x in enumerate(actual) if x == 3]\n",
    "    n_correct = sum(preds[i] == 3 for i in indices)\n",
    "    n_total = len(indices)\n",
    "    acc = n_correct / n_total\n",
    "    acc_3.append(acc)\n",
    "\n",
    "\n",
    "print(f'{round(np.mean(acc_0), 3)} ({round(min(acc_0), 3)} - {round(max(acc_0), 3)})')\n",
    "print(f'{round(np.mean(acc_1), 3)} ({round(min(acc_1), 3)} - {round(max(acc_1), 3)})')\n",
    "print(f'{round(np.mean(acc_2), 3)} ({round(min(acc_2), 3)} - {round(max(acc_2), 3)})')\n",
    "print(f'{round(np.mean(acc_3), 3)} ({round(min(acc_3), 3)} - {round(max(acc_3), 3)})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_path+'multiclass-results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_path+'multiclass-results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.create_sheet()\n",
    "\n",
    "row = 2\n",
    "metrics_list = ['accuracy', 'balanced accuracy']\n",
    "for i, column in enumerate(['model'] + metrics_list):\n",
    "    sheet.cell(row=1, column=i+1).value = column\n",
    "\n",
    "for model in ['xgboost']:\n",
    "    for model_type in ['train', 'test']:\n",
    "        try:\n",
    "            sheet.cell(row=row, column=1).value = f'{model.upper()} {model_type.upper()}'\n",
    "            for j, metric in enumerate(metrics_list):\n",
    "                    scores = []\n",
    "                    for fold in range(len(results[model+'_'+model_type])):\n",
    "                        scores.append(results[model+'_'+model_type][fold][metric])\n",
    "                    \n",
    "                    sheet.cell(row=row, column=j+2).value = f'{round(np.mean(scores), 3)} ({round(min(scores), 3)} - {round(max(scores), 3)})'\n",
    "            row += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "wb.save(results_path + 'multiclass-scores.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CONFUSION MATRIX'''\n",
    "\n",
    "def plot_confusion_matrix(target, preds, classifier):\n",
    "    plt.figure(dpi=300)\n",
    "    sns.heatmap(metrics.confusion_matrix(\n",
    "        target, preds),\n",
    "        annot=True, xticklabels=labels, \n",
    "        yticklabels=labels, fmt='g', cmap=\"flare\")\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    #plt.title(f'{classifier} Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(target, preds, classifier, labels):\n",
    "    cm = metrics.confusion_matrix(target, preds)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(dpi=300)\n",
    "    sns.heatmap(cm_normalized, annot=True, xticklabels=labels, yticklabels=labels, fmt='.2%', cmap=\"flare\")\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    #plt.title(f'{classifier} Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "labels = ['Banfora', 'Kisumu', 'Ngoussu', 'VK7']\n",
    "\n",
    "target = []\n",
    "preds = []\n",
    "for index in range(len(train_track_preds)):\n",
    "    target += list(train_track_preds[index]['track-target'])\n",
    "    preds += list(train_track_preds[index]['track-preds'])\n",
    "plot_confusion_matrix(target, preds, 'XGBoost (train)', labels)\n",
    "\n",
    "target = []\n",
    "preds = []\n",
    "for index in range(len(test_track_preds)):\n",
    "    target += list(test_track_preds[index]['track-target'])\n",
    "    preds += list(test_track_preds[index]['track-preds'])\n",
    "plot_confusion_matrix(target, preds, 'XGBoost (test)', labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''EXCEL FILE OF ALL FOLD SCORES'''\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "for key in ['xgboost_train','xgboost_test']:\n",
    "    sheet = wb.create_sheet(key.upper())\n",
    "    columns = ['fold', 'test trials', 'train trials', 'accuracy', 'balanced accuracy']\n",
    "    for i, column in enumerate(columns):\n",
    "        sheet.cell(row=1, column=i+1).value = column\n",
    "\n",
    "        for row in range(len(results[key])):\n",
    "            if column not in ['fold', 'test trials', 'train trials']:\n",
    "                sheet.cell(row=row+2, column=i+1).value = results[key][row][column]\n",
    "        \n",
    "            elif column == 'fold':\n",
    "                sheet.cell(row=row+2, column=i+1).value = row\n",
    "\n",
    "            elif column == 'test trials':\n",
    "                train = folds[row]\n",
    "                all_ids = df_train_target['TrialID'].unique()\n",
    "                sheet.cell(row=row+2, column=i+1).value = str([x for x in all_ids if x not in train])\n",
    "\n",
    "            elif column == 'train trials':\n",
    "                sheet.cell(row=row+2, column=i+1).value = str(folds[row])\n",
    "\n",
    "wb.save(results_path + \"multiclass-all-folds.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94d2f0a72470cbfa5bf040eb69ac68719f2f2aa4e7158c9baf86afd24ebc134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
